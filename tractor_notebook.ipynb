{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python378jvsc74a57bd057baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6",
   "display_name": "Python 3.7.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Dealer.shuffle and cards2str functions\n",
    "\n",
    "import numpy as np\n",
    "from rlcard.games.tractor.dealer import TractorDealer\n",
    "\n",
    "np_random = np.random.RandomState()\n",
    "dealer = TractorDealer(np_random)\n",
    "dealer.shuffle()\n",
    "print(dealer.deck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Judger.playable_cards_from_hand function\n",
    "\n",
    "from rlcard.games.tractor.judger import TractorJudger\n",
    "\n",
    "print(TractorJudger.playable_cards_from_hand(['AD','9H','KC','KC','AC','AC','3D','3D','7H','BJ','BJ','RJ','RJ','AD','2D','2D','2H','2H','2S','2S','AS','AS','AD','AD']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Player.available_actions function\n",
    "\n",
    "from rlcard.games.tractor import Game\n",
    "import numpy as np\n",
    "\n",
    "game = Game()\n",
    "game.init_game()\n",
    "\n",
    "print(game.players[0].current_hand)\n",
    "playable_cards = game.judger.get_playable_cards(game.players[0])\n",
    "print(playable_cards)\n",
    "played_cards = max(playable_cards, key=len)\n",
    "\n",
    "game.players[0].played_cards = played_cards\n",
    "#game.players[0].played_cards = cards2str([game.players[0].current_hand[10]])\n",
    "print(game.players[0].played_cards)\n",
    "print(game.players[1].current_hand)\n",
    "available_actions = game.players[1].available_actions(game.players[0], game.judger, game.round)\n",
    "print(available_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Test for round.calc_score_in_round\n",
    "'''\n",
    "import numpy as np\n",
    "from rlcard.games.tractor import Round\n",
    "\n",
    "\n",
    "np_random = np.random.RandomState()\n",
    "round = Round(np_random)\n",
    "\n",
    "round.current_round = [['5S','TS'], ['5H','5S'], ['3S'], ['KH']]\n",
    "print(round.calc_score_in_round())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' generate action space\n",
    "'''\n",
    "import numpy as np\n",
    "from rlcard.games.tractor import Dealer, Judger\n",
    "\n",
    "np_random = np.random.RandomState()\n",
    "dealer = Dealer(np)\n",
    "\n",
    "cards = dealer.deck\n",
    "# print(cards)\n",
    "\n",
    "actions = Judger.playable_cards_from_hand(cards)\n",
    "actions.sort()\n",
    "\n",
    "actions.extend(['pass', 'pass_score'])\n",
    "print(actions)\n",
    "# print(len(actions))\n",
    "\n",
    "action_index = {','.join(actions[x]):x for x in range(len(actions))}\n",
    "print(action_index)\n",
    "print(len(action_index.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Test Game class\n",
    "'''\n",
    "from rlcard.games.tractor import Game\n",
    "\n",
    "game = Game()\n",
    "state, player_id = game.init_game()\n",
    "\n",
    "print(state, \"\\r\\n\")\n",
    "\n",
    "while not game.is_over():\n",
    "     state, next_id = game.step(state['actions'][0])\n",
    "     print(state, \"\\r\\n\")\n",
    "\n",
    "print(state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' Test Game class perf\n",
    "'''\n",
    "from tqdm import tqdm\n",
    "from rlcard.games.tractor import Game\n",
    "import cProfile\n",
    "\n",
    "def run():\n",
    "    for iter in tqdm(range(1000)):\n",
    "        state, player_id = game.init_game()\n",
    "        while not game.is_over():\n",
    "            state, next_id = game.step(state['actions'][0])\n",
    "\n",
    "game = Game()\n",
    "cProfile.run('run()')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' test Tractor env\n",
    "'''\n",
    "\n",
    "''' An example of learning a NFSP Agent on Tractor\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import rlcard\n",
    "\n",
    "# Make environment\n",
    "env = rlcard.make('tractor', config={'seed': 0})\n",
    "\n",
    "env.reset()\n",
    "\n",
    "env.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CARD_RANK_STR = ['3H', '4H', '5H', '6H', '7H', '8H', '9H', 'TH', 'JH', 'QH', 'KH', 'AH',\n",
    "            '3C', '4C', '5C', '6C', '7C', '8C', '9C', 'TC', 'JC', 'QC', 'KC', 'AC',\n",
    "            '3D', '4D', '5D', '6D', '7D', '8D', '9D', 'TD', 'JD', 'QD', 'KD', 'AD',\n",
    "            '3S', '4S', '5S', '6S', '7S', '8S', '9S', 'TS', 'JS', 'QS', 'KS', 'AS',\n",
    "            '2H', '2C', '2D', '2S', 'BJ', 'RJ']\n",
    "\n",
    "print({CARD_RANK_STR[x] : x for x in range(len(CARD_RANK_STR))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    ''' An example of learning a NFSP Agent on Tractor\n",
    "    '''\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    import rlcard\n",
    "    from rlcard.agents import NFSPAgent\n",
    "    from rlcard.agents import RandomAgent\n",
    "    from rlcard.utils import set_global_seed, tournament\n",
    "    from rlcard.utils import Logger\n",
    "\n",
    "    # Make environment\n",
    "    env = rlcard.make('tractor', config={'seed': 0})\n",
    "    eval_env = rlcard.make('tractor', config={'seed': 0})\n",
    "\n",
    "    # Set the iterations numbers and how frequently we evaluate the performance\n",
    "    evaluate_every = 1000\n",
    "    evaluate_num = 1000\n",
    "    # episode_num = 100000\n",
    "    episode_num = 2000\n",
    "\n",
    "    # The intial memory size\n",
    "    memory_init_size = 1000\n",
    "\n",
    "    # Train the agent every X steps\n",
    "    train_every = 64\n",
    "\n",
    "    # The paths for saving the logs and learning curves\n",
    "    log_dir = './experiments/tractor_nfsp_result/'\n",
    "\n",
    "    # Set a global seed\n",
    "    set_global_seed(0)\n",
    "\n",
    "    # Mitigation for gpu memory issue\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    # config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "    with tf.Session(config=config) as sess:\n",
    "    # with tf.Session() as sess:\n",
    "        \n",
    "        # Initialize a global step\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "        # Set up the agents\n",
    "        agents = []\n",
    "        for i in range(env.player_num):\n",
    "            agent = NFSPAgent(sess,\n",
    "                            scope='nfsp' + str(i),\n",
    "                            action_num=env.action_num,\n",
    "                            state_shape=env.state_shape,\n",
    "                            hidden_layers_sizes=[512,1024,2048,1024,512],\n",
    "                            #hidden_layers_sizes=[512,1024,512],\n",
    "                            #   hidden_layers_sizes=[64],\n",
    "                            anticipatory_param=0.5,\n",
    "                            batch_size=256,\n",
    "                            rl_learning_rate=0.00005,\n",
    "                            sl_learning_rate=0.00001,\n",
    "                            min_buffer_size_to_learn=memory_init_size,\n",
    "                            q_replay_memory_size=int(1e5),\n",
    "                            #   q_replay_memory_size=int(1000),\n",
    "                            q_replay_memory_init_size=memory_init_size,\n",
    "                            train_every = train_every,\n",
    "                            q_train_every=train_every,\n",
    "                            q_batch_size=256,\n",
    "                            q_mlp_layers=[512,1024,2048,1024,512],\n",
    "                            #   q_mlp_layers=[512,1024,512],\n",
    "                            #   q_mlp_layers=[64],\n",
    "                            reservoir_buffer_capacity=int(1e3))\n",
    "            agents.append(agent)\n",
    "        random_agent = RandomAgent(action_num=eval_env.action_num)\n",
    "\n",
    "        env.set_agents(agents)\n",
    "        eval_env.set_agents([agents[0], random_agent, random_agent, random_agent])\n",
    "\n",
    "        # Initialize global variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Init a Logger to plot the learning curvefrom rlcard.agents.random_agent import RandomAgent\n",
    "\n",
    "        logger = Logger(log_dir)\n",
    "\n",
    "        for episode in tqdm(range(episode_num)):\n",
    "            # First sample a policy for the episode\n",
    "            for agent in agents:\n",
    "                agent.sample_episode_policy()\n",
    "\n",
    "            # Generate data from the environment\n",
    "            trajectories, _ = env.run(is_training=True)\n",
    "\n",
    "            # Feed transitions into agent memory, and train the agent\n",
    "            for i in range(env.player_num):\n",
    "                for ts in trajectories[i]:\n",
    "                    agents[i].feed(ts)\n",
    "\n",
    "            # Evaluate the performance. Play with random agents.\n",
    "            if episode % evaluate_every == 0:\n",
    "                logger.log_performance(env.timestep, tournament(eval_env, evaluate_num)[0])\n",
    "\n",
    "        # Close files in the logger\n",
    "        logger.close_files()\n",
    "\n",
    "        # Plot the learning curve\n",
    "        logger.plot('NFSP')\n",
    "        \n",
    "        # Save model\n",
    "        save_dir = 'models/tractor_nfsp'\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, os.path.join(save_dir, 'model'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' perf test of E2E training\n",
    "'''\n",
    "\n",
    "import cProfile\n",
    "\n",
    "cProfile.run('train()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}